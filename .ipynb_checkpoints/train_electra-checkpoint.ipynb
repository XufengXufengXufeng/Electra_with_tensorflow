{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from algorithms.encoder import transformer_encoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from copy import copy,deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from random import shuffle,sample\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader(object):\n",
    "    def __init__(self,loc,max_len=512,batch_size=16):\n",
    "        self.f = open(loc)\n",
    "        self.loc = loc\n",
    "        self.batch_size = batch_size\n",
    "        self.result_buffer = self.f.readlines(max_len*self.batch_size)\n",
    "        self.buffer_len = len(self.result_buffer)\n",
    "        self.max_len = max_len\n",
    "    def read(self):\n",
    "        end_flag = False\n",
    "        if self.buffer_len > self.batch_size:\n",
    "            result = self.result_buffer[:self.batch_size]\n",
    "            self.result_buffer = np.delete(self.result_buffer,np.arange(self.batch_size)).tolist()\n",
    "            self.buffer_len = len(self.result_buffer)\n",
    "        else:\n",
    "            self.result_buffer += self.f.readlines(self.max_len*self.batch_size)\n",
    "            if len(self.result_buffer)==self.buffer_len:# 到头了没有读到新数据\n",
    "                result = self.result_buffer\n",
    "                end_flag = True\n",
    "                self.f.close()\n",
    "            else:\n",
    "                for _ in range(self.batch_size):\n",
    "                    self.result_buffer += self.f.readlines(self.max_len*self.batch_size)\n",
    "                    self.buffer_len = len(self.result_buffer)\n",
    "                    if self.buffer_len > self.batch_size:\n",
    "                        result = self.result_buffer[:self.batch_size]\n",
    "                        self.result_buffer = np.delete(self.result_buffer,np.arange(self.batch_size)).tolist()\n",
    "                        break\n",
    "                if self.buffer_len < self.batch_size:\n",
    "                    result = self.result_buffer\n",
    "                    self.buffer_len = 0\n",
    "                    self.result_buffer = [] \n",
    "        return [np.array(l.strip(\"\\n\").split(\",\")).astype(\"int\").tolist() for l in result],end_flag\n",
    "    def close(self):\n",
    "        self.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = Reader(\"data/processed_data/train.txt\",3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3293,\n",
       "  948,\n",
       "  3979,\n",
       "  6554,\n",
       "  4391,\n",
       "  169,\n",
       "  7766,\n",
       "  8832,\n",
       "  4398,\n",
       "  4921,\n",
       "  169,\n",
       "  8832,\n",
       "  5310,\n",
       "  169,\n",
       "  10966,\n",
       "  10966],\n",
       " [11229,\n",
       "  8500,\n",
       "  8757,\n",
       "  4981,\n",
       "  3138,\n",
       "  7766,\n",
       "  2674,\n",
       "  169,\n",
       "  8038,\n",
       "  1365,\n",
       "  6805,\n",
       "  3328,\n",
       "  7349,\n",
       "  827,\n",
       "  8968,\n",
       "  9021,\n",
       "  11605],\n",
       " [4229,\n",
       "  8986,\n",
       "  3626,\n",
       "  7854,\n",
       "  6805,\n",
       "  4861,\n",
       "  11155,\n",
       "  8832,\n",
       "  4398,\n",
       "  4921,\n",
       "  6188,\n",
       "  11705,\n",
       "  169,\n",
       "  690,\n",
       "  3819,\n",
       "  418,\n",
       "  11605,\n",
       "  11605]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx.read()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicts(char2id_loc,id2char_loc):\n",
    "    with open(char2id_loc,\"r\") as f:\n",
    "        char2id = json.load(f)\n",
    "    with open(id2char_loc,\"r\") as f:\n",
    "        id2char = json.load(f)\n",
    "    return char2id,id2char\n",
    "def tokenizer_producer(dicts,already=True):\n",
    "    if already:\n",
    "        def tokenizer(text):\n",
    "            result = [int(l) for l in text.strip(\"\\n\").split(\",\")]\n",
    "            return result\n",
    "        return tokenizer\n",
    "    def tokenizer(text):\n",
    "        result = [dicts.get(t,0) for t in text]\n",
    "        return result\n",
    "    return tokenizer\n",
    "def sample_positions(tokenized,max_len,mask_index):\n",
    "    padded,position_indeces,target_word_indeces,mask_values = [],[],[],[]\n",
    "    sample_size = np.random.choice(np.arange(min([len(s) for s in tokenized])))\n",
    "    for i,d in enumerate(tokenized):\n",
    "        d = d[:max_len]\n",
    "        row_indeces,row_targets = [],[]\n",
    "        randomeds = np.random.choice(np.arange(len(d)),size=sample_size,replace=False)\n",
    "        for j,randomed in enumerate(randomeds):\n",
    "            row_indeces.append([i,randomed])\n",
    "            row_targets.append([i,j,d[randomed]])\n",
    "        position_indeces.append(row_indeces)\n",
    "        target_word_indeces.append(row_targets)\n",
    "        pad_len = max_len-len(d)\n",
    "        padded.append((d+[0]*pad_len)[:max_len])\n",
    "        mask_values.append([mask_index]*sample_size)\n",
    "    return padded,position_indeces,target_word_indeces,mask_values\n",
    "def data_batcher_producer(train_loc,max_len,batch_size,mask_index):\n",
    "    def data_batcher():\n",
    "        tx = Reader(train_loc,max_len,batch_size)\n",
    "        data_list,end_flag = tx.read()\n",
    "        padded,position_indeces,target_word_indeces,mask_values = sample_positions(\n",
    "            data_list,max_len,mask_index)\n",
    "        yield end_flag,padded,position_indeces,target_word_indeces,mask_values\n",
    "    return data_batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id_loc = \"data/processed_data/char2id.json\"\n",
    "id2char_loc = \"data/processed_data/id2char.json\"\n",
    "char2id,_=get_dicts(char2id_loc,id2char_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc = \"data/processed_data/train.txt\"\n",
    "max_len = 512\n",
    "batch_size = 3\n",
    "mask_index = char2id[\"mask\"]\n",
    "db = data_batcher_producer(train_loc,max_len,batch_size,mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-b90df0ee928f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "for d in db:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batcher = data_batcher_producer(512,16,tokenizer,get_dicts()[0][\"mask\"])\n",
    "db = data_batcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1185, 6665, 418, 2912, 5078, 513, 9463, 9716, 5020, 1731, 513, 9716, 797, 513, 3110, 3110]\n",
      "shit\n"
     ]
    }
   ],
   "source": [
    "for d in db:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-b8e30b5c1995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 11, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(d[3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(sess,tf_model,access_dict):\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "#     try: \n",
    "#         os.mkdir(\"{}\".format(tf_model))\n",
    "#     except:\n",
    "#         pass\n",
    "    logdir = \"{}\".format(tf_model)\n",
    "    #from tensorflow.compat.v1.graph_util import convert_variables_to_constants\n",
    "    #graph = convert_variables_to_constants(sess,sess.graph_def,[\"input\",\"prediction\"])\n",
    "#    tf.io.write_graph(graph,logdir,\"tf_model.pb\",as_text=False)\n",
    "    try:\n",
    "        tf.saved_model.simple_save(sess,\n",
    "                logdir,\n",
    "                inputs={\"input\": access_dict[\"input\"],\n",
    "                        \"training\":access_dict[\"training\"],\n",
    "                       \"position_indeces\":access_dict[\"position_indeces\"],\n",
    "                       \"target_word_indeces\":access_dict[\"target_word_indeces\"],\n",
    "                       \"mask_values\":access_dict[\"mask_values\"],\n",
    "                       \"peep\":access_dict[\"peep\"]},\n",
    "                outputs={\"outputs\": access_dict[\"outputs\"]})\n",
    "    except:\n",
    "        shutil.rmtree(logdir)\n",
    "        tf.saved_model.simple_save(sess,\n",
    "                logdir,\n",
    "                inputs={\"input\": access_dict[\"input\"],\n",
    "                        \"training\":access_dict[\"training\"],\n",
    "                       \"position_indeces\":access_dict[\"position_indeces\"],\n",
    "                       \"target_word_indeces\":access_dict[\"target_word_indeces\"],\n",
    "                       \"mask_values\":access_dict[\"mask_values\"],\n",
    "                       \"peep\":access_dict[\"peep\"]},\n",
    "                outputs={\"outputs\": access_dict[\"outputs\"]})\n",
    "    print(\"{}\".format(tf_model))\n",
    "def get_interpreter(model_path):\n",
    "    parser = tf.contrib.predictor.from_saved_model(model_path)\n",
    "    def interpret(inputs):\n",
    "        return parser({\"input\":inputs,\"training\":False,\n",
    "                      \"position_indeces\":[[[0,0]]],\n",
    "                      \"target_word_indeces\":[[[0,0,0]]],\n",
    "                      \"mask_values\":[[0]],\n",
    "                      \"peep\":False})[\"outputs\"]\n",
    "    return interpret\n",
    "def bot_thought(interpreter,tokenizer,max_len=400):\n",
    "    def bot_say(inputs):\n",
    "        if type(inputs).__name__==\"list\":\n",
    "            sentences = []\n",
    "            tokenized = [tokenizer(s) for s in inputs]\n",
    "            for t in tokenized:\n",
    "                pad_len = max_len-len(t)\n",
    "                sentences.append((t+[0]*pad_len)[:max_len])\n",
    "\n",
    "            ans = interpreter(sentences)\n",
    "\n",
    "            #ans = interpreter({\"inputs\":inputs})[\"predictions\"]\n",
    "            #print(ans)\n",
    "        else:\n",
    "            tokenized = tokenizer(inputs)\n",
    "            pad_len = max_len-len(tokenized)\n",
    "            tokenized = (tokenized+[0]*pad_len)[:max_len]\n",
    "            ans = interpreter([tokenized])\n",
    "        return ans\n",
    "    return bot_say"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_encoder_producer(embedding_layer,projector,g_transformer):\n",
    "    def encode(inputs):\n",
    "        projected = projector(embedding_layer.embeddings)\n",
    "        embedded = tf.nn.embedding_lookup(projected,inputs)\n",
    "        encoded = g_transformer(embedded)\n",
    "        return projected,encoded\n",
    "    return encode\n",
    "def discriminitor_encoder_producer(embedding_layer,d_transformer):\n",
    "    def encode(inputs):\n",
    "        embeded = embedding_layer(inputs)\n",
    "        encoded = d_transformer(embeded)\n",
    "        return encoded\n",
    "    return encode\n",
    "def electra_producer(vocab_size,embedding_size,generator_size,\n",
    "                    gn_blocks,gseq_length,gn_heads,gff_filter_size,g_dev,\n",
    "                    dn_blocks,dseq_length,dn_heads,dff_filter_size,d_dev,mask_index):\n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        embedding_layer(tf.constant([0]))\n",
    "        embedding_projector = tf.keras.layers.Dense(generator_size)\n",
    "    with tf.device(g_dev):\n",
    "        g_transformer = transformer_encoder(gn_blocks,gseq_length,generator_size,generator_size,\n",
    "                                            gn_heads,gff_filter_size,name=\"gt\")\n",
    "        generator_encoder = generator_encoder_producer(embedding_layer,embedding_projector,g_transformer)\n",
    "    with tf.device(d_dev):\n",
    "        d_transformer = transformer_encoder(dn_blocks,dseq_length,embedding_size,embedding_size,\n",
    "                                            dn_heads,dff_filter_size,name=\"dt\")\n",
    "        discriminitor_encoder = discriminitor_encoder_producer(embedding_layer,d_transformer)\n",
    "        output_layer = tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "    def flow(inputs,position_indeces,target_word_indeces,training):\n",
    "        losses,layers = {\"generator_loss\":0,\"discriminitor_loss\":0},{}\n",
    "        \n",
    "        if training==False:\n",
    "            print(training==False)\n",
    "            d_encoded = discriminitor_encoder(inputs)\n",
    "            layers[\"d_encoded\"] = d_encoded\n",
    "            return losses,layers\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            mask_count = tf.shape(position_indeces)[1]\n",
    "            corrupted = tf.tensor_scatter_nd_update(inputs,position_indeces,[[mask_index]*mask_count]*batch_size)\n",
    "            g_projected,g_encoded = generator_encoder(inputs)\n",
    "            masked_g_encoded = tf.gather_nd(g_encoded,position_indeces)\n",
    "            generated = tf.nn.softmax(\n",
    "                tf.transpose(\n",
    "                    tf.matmul(g_projected,masked_g_encoded,transpose_b=True),[0,2,1]\n",
    "                )\n",
    "            )\n",
    "            print(\"shit\")\n",
    "            losses[\"generator_loss\"] = tf.reduce_sum(-tf.math.log(\n",
    "                tf.gather_nd(generated,target_word_indeces)+1e-6))\n",
    "            replaced = tf.tensor_scatter_nd_update(\n",
    "                inputs,position_indeces,tf.cast(tf.math.argmax(generated,axis=-1),tf.int32)\n",
    "            )\n",
    "            labels = tf.cast(tf.clip_by_value(tf.abs(inputs-replaced),0,1),tf.float32)\n",
    "            target_signs = labels*-2+1\n",
    "\n",
    "            d_encoded = discriminitor_encoder(replaced)\n",
    "            layers[\"d_encoded\"] = d_encoded\n",
    "            d_out = output_layer(d_encoded)\n",
    "            pre_d_loss = (tf.squeeze(d_out)-labels)*target_signs\n",
    "            losses[\"discriminitor_loss\"] = tf.reduce_sum(tf.math.log1p(pre_d_loss+1e-6))\n",
    "            return losses,layers\n",
    "        \n",
    "    return flow\n",
    "def cpu_or_gpu0(op):\n",
    "    if op.type == \"Variable\":\n",
    "        return \"/CPU:0\"\n",
    "    else:\n",
    "        return \"/GPU:0\"\n",
    "def cpu_or_gpu1(op):\n",
    "    if op.type == \"Variable\":\n",
    "        return \"/CPU:0\"\n",
    "    else:\n",
    "        return \"/GPU:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(vocab_size,embedding_size,generator_size,\n",
    "                    gn_blocks,gseq_length,gn_heads,gff_filter_size,g_dev,\n",
    "                    dn_blocks,dseq_length,dn_heads,dff_filter_size,d_dev,mask_index,\n",
    "                d_factor,learning_rate):\n",
    "    g = tf.Graph()\n",
    "    tf.reset_default_graph()\n",
    "    with g.as_default():\n",
    "        access_dict = {}\n",
    "        access_dict[\"input\"] = tf.placeholder(tf.int32,shape=[None,gseq_length],name=\"input\")\n",
    "        access_dict[\"training\"] = tf.placeholder(tf.bool,shape=[],name=\"training\")\n",
    "        access_dict[\"peep\"] = tf.placeholder(tf.bool,shape=[],name=\"peep\")\n",
    "        access_dict[\"mask_values\"] = tf.placeholder(tf.int32,shape=[None,None],name=\"mask_values\")\n",
    "        access_dict[\"position_indeces\"] = tf.placeholder(tf.int32,shape=[None,None,None],name=\"position_indeces\")\n",
    "        access_dict[\"target_word_indeces\"] = tf.placeholder(tf.int32,shape=[None,None,None],name=\"target_word_indeces\")\n",
    "        \n",
    "        with tf.device(\"/CPU:0\"):\n",
    "            embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "            embedding_layer(tf.constant([0]))\n",
    "            embedding_projector = tf.keras.layers.Dense(generator_size)\n",
    "        with tf.device(g_dev):\n",
    "            g_transformer = transformer_encoder(gn_blocks,gseq_length,generator_size,generator_size,\n",
    "                                                gn_heads,gff_filter_size,name=\"gt\")\n",
    "            generator_encoder = generator_encoder_producer(embedding_layer,embedding_projector,g_transformer)\n",
    "        with tf.device(d_dev):\n",
    "            d_transformer = transformer_encoder(dn_blocks,dseq_length,embedding_size,generator_size,\n",
    "                                                dn_heads,dff_filter_size,name=\"dt\")\n",
    "            discriminitor_encoder = discriminitor_encoder_producer(embedding_layer,d_transformer)\n",
    "            output_layer = tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
    "        losses,layers = {\"generator_loss\":0,\"discriminitor_loss\":0},{}\n",
    "\n",
    "        if access_dict[\"training\"]==False:\n",
    "            print(training==False)\n",
    "            d_encoded = discriminitor_encoder(access_dict[\"input\"])\n",
    "            layers[\"d_encoded\"] = d_encoded\n",
    "        else:\n",
    "            corrupted = tf.tensor_scatter_nd_update(\n",
    "                access_dict[\"input\"],access_dict[\"position_indeces\"],\n",
    "                access_dict[\"mask_values\"])\n",
    "            if access_dict[\"peep\"]==False:\n",
    "                g_projected,g_encoded = generator_encoder(corrupted)\n",
    "            else:\n",
    "                g_projected,g_encoded = generator_encoder(access_dict[\"input\"])\n",
    "            masked_g_encoded = tf.gather_nd(g_encoded,access_dict[\"position_indeces\"])\n",
    "            generated = tf.nn.softmax(\n",
    "                tf.transpose(\n",
    "                    tf.matmul(g_projected,masked_g_encoded,transpose_b=True),[0,2,1]\n",
    "                )\n",
    "            )\n",
    "            print(generated.shape)\n",
    "            print(\"shit\")\n",
    "            losses[\"generator_loss\"] = tf.reduce_sum(-tf.math.log(\n",
    "                tf.gather_nd(generated,access_dict[\"target_word_indeces\"])+1e-6))\n",
    "            \n",
    "            replaced = tf.tensor_scatter_nd_update(\n",
    "                access_dict[\"input\"],\n",
    "                access_dict[\"position_indeces\"],tf.cast(tf.math.argmax(generated,axis=-1),tf.int32)\n",
    "            )\n",
    "            labels = tf.cast(tf.clip_by_value(tf.abs(access_dict[\"input\"]-replaced),0,1),tf.float32)\n",
    "            target_signs = labels*-2+1\n",
    "\n",
    "            d_encoded = discriminitor_encoder(replaced)\n",
    "            layers[\"d_encoded\"] = d_encoded\n",
    "            d_out = output_layer(d_encoded)\n",
    "            pre_d_loss = (tf.squeeze(d_out)-labels)*target_signs\n",
    "            losses[\"discriminitor_loss\"] = tf.reduce_sum(tf.math.log1p(pre_d_loss+1e-6))\n",
    "        access_dict[\"outputs\"] = layers[\"d_encoded\"]        \n",
    "        access_dict[\"losses\"] = losses[\"generator_loss\"]+d_factor*losses[\"discriminitor_loss\"]\n",
    "        access_dict[\"g_loss\"] = losses[\"generator_loss\"]\n",
    "        access_dict[\"d_loss\"] = losses[\"discriminitor_loss\"]\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        access_dict[\"training_op\"] = optimizer.minimize(access_dict[\"losses\"])\n",
    "        access_dict[\"init\"] = tf.global_variables_initializer()\n",
    "        print(\"encoded\",access_dict[\"outputs\"].shape)\n",
    "    return g, access_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_graph(vocab_size,embedding_size,generator_size,\n",
    "                    gn_blocks,gseq_length,gn_heads,gff_filter_size,g_dev,\n",
    "                    dn_blocks,dseq_length,dn_heads,dff_filter_size,d_dev,mask_index,\n",
    "                d_factor,learning_rate):\n",
    "    g = tf.Graph()\n",
    "    tf.reset_default_graph()\n",
    "    with g.as_default():\n",
    "        access_dict = {}\n",
    "        access_dict[\"input\"] = tf.placeholder(tf.int32,shape=[None,gseq_length],name=\"input\")\n",
    "        access_dict[\"training\"] = tf.placeholder(tf.bool,shape=[],name=\"training\")\n",
    "        access_dict[\"position_indeces\"] = tf.placeholder(tf.int32,shape=[None,None,None],name=\"position_indeces\")\n",
    "        access_dict[\"target_word_indeces\"] = tf.placeholder(tf.int32,shape=[None,None,None],name=\"target_word_indeces\")\n",
    "        \n",
    "        electra = electra_producer(vocab_size,embedding_size,generator_size,\n",
    "                    gn_blocks,gseq_length,gn_heads,gff_filter_size,g_dev,\n",
    "                    dn_blocks,dseq_length,dn_heads,dff_filter_size,d_dev,mask_index)\n",
    "        losses,access_dict[\"outputs\"] = electra(\n",
    "            access_dict[\"input\"],access_dict[\"position_indeces\"],access_dict[\"target_word_indeces\"],\n",
    "            access_dict[\"training\"]\n",
    "        )\n",
    "        access_dict[\"losses\"] = losses[\"generator_loss\"]+d_factor*losses[\"discriminitor_loss\"]\n",
    "        access_dict[\"g_loss\"] = losses[\"generator_loss\"]\n",
    "        access_dict[\"d_loss\"] = losses[\"discriminitor_loss\"]\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        access_dict[\"training_op\"] = optimizer.minimize(access_dict[\"losses\"])\n",
    "        access_dict[\"init\"] = tf.global_variables_initializer()\n",
    "        print(\"encoded\",access_dict[\"outputs\"][\"d_encoded\"].shape)\n",
    "    return g, access_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shit\n",
      "encoded (?, 400, 100)\n"
     ]
    }
   ],
   "source": [
    "char2id,id2char = get_dicts(all_text)\n",
    "vocab_size = len(char2id)\n",
    "embedding_size = 100\n",
    "generator_size = 50\n",
    "gn_blocks = 2\n",
    "gseq_length = 400\n",
    "gn_heads = 6\n",
    "gff_filter_size = 150\n",
    "g_dev = \"/GPU:0\"\n",
    "dn_blocks = 2\n",
    "dseq_length = 400\n",
    "dn_heads = 6\n",
    "dff_filter_size = 300\n",
    "d_dev = \"/GPU:1\"\n",
    "mask_index = char2id[\"mask\"]\n",
    "d_factor = 20\n",
    "learning_rate = 1e-3\n",
    "graph,access_dict = build_graph(vocab_size,embedding_size,generator_size,\n",
    "                    gn_blocks,gseq_length,gn_heads,gff_filter_size,g_dev,\n",
    "                    dn_blocks,dseq_length,dn_heads,dff_filter_size,d_dev,mask_index,\n",
    "                d_factor,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tf_model(graph,access_dict,data_batcher,tf_model_name,epochs=5):\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        \n",
    "        access_dict[\"init\"].run()\n",
    "        losses,g_losses,d_losses = [],[],[]\n",
    "        #\"\"\"\n",
    "        for e in  range(epochs):\n",
    "            db = data_batcher(e)\n",
    "            tmp_losses,tmp_g_losses,tmp_d_losses,mask_counts = [],[],[],[]\n",
    "            mask_count = 0\n",
    "            for i,padded,position_indeces,target_word_indeces,mask_values,peep in db:\n",
    "                if (len(position_indeces[0])==0)|(len(target_word_indeces[0])==0):\n",
    "                    continue\n",
    "                feed = {\"input\":padded,\"position_indeces\":position_indeces,\n",
    "                        \"target_word_indeces\":target_word_indeces,\"mask_values\":mask_values,\n",
    "                       \"training\":True,\"peep\":peep}\n",
    "                loss,g_loss,d_loss,_ = sess.run([\n",
    "                    access_dict[\"losses\"],\n",
    "                                   access_dict[\"g_loss\"],access_dict[\"d_loss\"],access_dict[\"training_op\"]],\n",
    "                                 feed_dict = {access_dict[k]:feed[k] for k in feed.keys()})\n",
    "                \n",
    "                tmp_losses.append(float(loss.mean()))\n",
    "                tmp_g_losses.append(float(g_loss.mean()))\n",
    "                tmp_d_losses.append(float(d_loss.mean()))\n",
    "                mask_counts.append(len(mask_values[0]))\n",
    "                if i%22==0:\n",
    "                    losses.append(float(np.mean(tmp_losses)))\n",
    "                    g_losses.append(float(np.mean(tmp_g_losses)))\n",
    "                    d_losses.append(float(np.mean(tmp_d_losses)))\n",
    "                    if mask_count < np.mean(mask_counts):\n",
    "                        mask_count = np.mean(mask_counts)\n",
    "                    tmp_losses,tmp_g_losses,tmp_d_losses,mask_counts = [],[],[],[]\n",
    "            print(\"epoch {}: loss is {:.2f} g_loss is {:.2f} d_loss is {:.2f} mask count is {:.2f}\"\n",
    "                      .format(e,losses[-1],g_losses[-1],d_losses[-1],mask_count))\n",
    "#         save_models(sess,access_dict[\"input\"],\n",
    "#                     access_dict[\"prediction\"],tf_model_name,w2id)\n",
    "        save_models(sess,tf_model_name,access_dict)\n",
    "        #file_writer = tf.summary.FileWriter('logs', sess.graph)\n",
    "    pd.Series(losses).clip(None,10).plot();\n",
    "    plt.show();\n",
    "    pd.Series(g_losses).clip(None,5).plot();\n",
    "    plt.show();\n",
    "    pd.Series(d_losses).clip(None,2).plot();\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, ?)\n",
      "shit\n",
      "encoded (?, 400, 512)\n",
      "epoch 0: loss is 1913.39 g_loss is 611.53 d_loss is 65.09 mask count is 8.35\n",
      "epoch 1: loss is 1898.49 g_loss is 604.48 d_loss is 64.70 mask count is 8.00\n",
      "epoch 2: loss is 2243.82 g_loss is 715.30 d_loss is 76.43 mask count is 7.20\n",
      "epoch 3: loss is 2003.64 g_loss is 631.81 d_loss is 68.59 mask count is 6.88\n",
      "epoch 4: loss is 2148.55 g_loss is 680.22 d_loss is 73.42 mask count is 7.68\n",
      "epoch 5: loss is 2029.40 g_loss is 642.98 d_loss is 69.32 mask count is 11.00\n",
      "epoch 6: loss is 1984.38 g_loss is 630.53 d_loss is 67.69 mask count is 7.59\n",
      "epoch 7: loss is 1679.82 g_loss is 537.23 d_loss is 57.13 mask count is 11.00\n",
      "epoch 8: loss is 2163.40 g_loss is 684.56 d_loss is 73.94 mask count is 7.25\n",
      "epoch 9: loss is 2979.71 g_loss is 1487.24 d_loss is 74.62 mask count is 7.59\n",
      "epoch 10: loss is 2755.52 g_loss is 1375.33 d_loss is 69.01 mask count is 7.05\n",
      "epoch 11: loss is 2638.32 g_loss is 1316.84 d_loss is 66.07 mask count is 7.65\n",
      "epoch 12: loss is 2472.74 g_loss is 1234.19 d_loss is 61.93 mask count is 22.00\n",
      "epoch 13: loss is 2717.23 g_loss is 1356.22 d_loss is 68.05 mask count is 7.26\n",
      "epoch 14: loss is 2690.21 g_loss is 1342.74 d_loss is 67.37 mask count is 7.48\n",
      "epoch 15: loss is 2528.55 g_loss is 1262.05 d_loss is 63.33 mask count is 7.19\n",
      "epoch 16: loss is 2572.69 g_loss is 1284.07 d_loss is 64.43 mask count is 8.48\n",
      "epoch 17: loss is 3326.81 g_loss is 1660.49 d_loss is 83.32 mask count is 9.67\n",
      "epoch 18: loss is 2423.38 g_loss is 1209.55 d_loss is 60.69 mask count is 9.00\n",
      "epoch 19: loss is 2969.01 g_loss is 1481.90 d_loss is 74.36 mask count is 8.10\n",
      "epoch 20: loss is 2475.37 g_loss is 1235.50 d_loss is 61.99 mask count is 7.85\n",
      "epoch 21: loss is 3042.24 g_loss is 1518.45 d_loss is 76.19 mask count is 7.59\n",
      "epoch 22: loss is 2547.86 g_loss is 1271.68 d_loss is 63.81 mask count is 7.85\n",
      "epoch 23: loss is 2237.93 g_loss is 1116.98 d_loss is 56.05 mask count is 11.00\n",
      "epoch 24: loss is 2906.36 g_loss is 1450.63 d_loss is 72.79 mask count is 9.00\n",
      "epoch 25: loss is 3256.50 g_loss is 1625.39 d_loss is 81.56 mask count is 7.60\n",
      "epoch 26: loss is 3212.21 g_loss is 1603.29 d_loss is 80.45 mask count is 7.55\n",
      "epoch 27: loss is 2680.98 g_loss is 1338.13 d_loss is 67.14 mask count is 9.00\n",
      "epoch 28: loss is 3281.41 g_loss is 1637.83 d_loss is 82.18 mask count is 7.70\n",
      "epoch 29: loss is 2227.58 g_loss is 1111.82 d_loss is 55.79 mask count is 9.00\n",
      "epoch 30: loss is 2744.45 g_loss is 1369.81 d_loss is 68.73 mask count is 9.00\n",
      "epoch 31: loss is 3091.34 g_loss is 1542.96 d_loss is 77.42 mask count is 7.27\n",
      "epoch 32: loss is 2451.05 g_loss is 1223.36 d_loss is 61.38 mask count is 11.00\n",
      "epoch 33: loss is 2978.86 g_loss is 1486.81 d_loss is 74.60 mask count is 8.00\n",
      "epoch 34: loss is 2494.10 g_loss is 1244.85 d_loss is 62.46 mask count is 7.79\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3e90839a68af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mdn_blocks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdseq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdn_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdff_filter_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 d_factor,learning_rate)\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain_tf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccess_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_batcher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-124bd4c941c5>\u001b[0m in \u001b[0;36mtrain_tf_model\u001b[0;34m(graph, access_dict, data_batcher, tf_model_name, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0maccess_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"losses\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                    access_dict[\"g_loss\"],access_dict[\"d_loss\"],access_dict[\"training_op\"]],\n\u001b[0;32m---> 20\u001b[0;31m                                  feed_dict = {access_dict[k]:feed[k] for k in feed.keys()})\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mtmp_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "char2id,id2char = get_dicts(all_text)\n",
    "vocab_size = len(char2id)\n",
    "embedding_size = 512\n",
    "generator_size = 512\n",
    "gn_blocks = 2\n",
    "gseq_length = 400\n",
    "gn_heads = 6\n",
    "gff_filter_size = 300\n",
    "g_dev = \"/GPU:0\"\n",
    "dn_blocks = 2\n",
    "dseq_length = 400\n",
    "dn_heads = 6\n",
    "dff_filter_size = 600\n",
    "d_dev = \"/GPU:1\"\n",
    "mask_index = char2id[\"mask\"]\n",
    "d_factor = 20\n",
    "learning_rate = 1e-3\n",
    "max_len = 400\n",
    "batch_size = 16\n",
    "tf_model_name = \"models/haha6\"\n",
    "epochs = 50\n",
    "tokenizer = tokenizer_producer(char2id)\n",
    "sentences = data[\"train\"].apply(lambda row:row.content+row.title,axis=1).tolist()\n",
    "tokenized = [tokenizer(s) for s in sentences]\n",
    "data_batcher = data_batcher_producer(max_len,batch_size,tokenized,char2id[\"mask\"])\n",
    "graph,access_dict = build_graph(vocab_size,embedding_size,generator_size,\n",
    "                    gn_blocks,gseq_length,gn_heads,gff_filter_size,g_dev,\n",
    "                    dn_blocks,dseq_length,dn_heads,dff_filter_size,d_dev,mask_index,\n",
    "                d_factor,learning_rate)\n",
    "train_tf_model(graph,access_dict,data_batcher,tf_model_name,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = get_interpreter(\"models/haha6\")\n",
    "bot_say = bot_thought(interpreter,tokenizer,400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = bot_say([\"'\\u3000\\u3000一个毛病是，80码加速的时候发动机抖动得厉害，放掉油门就不抖了。\\\n",
    "过了90码也不抖了。我估计是活塞磨损或是气门磨损了。不知道有没有遇到过这个情况的.。今天吓到我了\\\n",
    "，高速爬坡熄火！一下突然打不着。赶紧靠边停车，过几分钟又能打着了。\\\n",
    "估计是氧传感器的问题。 \\u3000\\u3000第二个毛病是，\\\n",
    "热车停车只要停车超过一个小时就打不着火，感觉是淹缸。钥匙拧开电门踩油门一两分钟在重复一次再打火才有可能打着火。\\\n",
    "不知道是喷油嘴的问题还是油路的问题或是电油泵的问题。'\"]).mean(axis=-1)\n",
    "b = bot_say(\"改装\").mean(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,b.T)/(np.sqrt(np.dot(a,a.T))*np.sqrt(np.dot(b,b.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
